{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "# Linear Regression one variable (manual implementation)\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Import necessary packages\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Set interactive backend \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Load data set \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "df = pd.DataFrame (X, columns= ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Select one feature\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['RM']] # Note: returns df comparing to  df['RM']\n",
    "df['target'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Review the data \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      RM  target\n",
      "0  6.575    24.0\n",
      "1  6.421    21.6\n",
      "2  7.185    34.7\n",
      "3  6.998    33.4\n",
      "4  7.147    36.2\n",
      "5  6.430    28.7\n",
      "6  6.012    22.9\n",
      "7  6.172    27.1\n",
      "8  5.631    16.5\n",
      "9  6.004    18.9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RM</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.284634</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.702617</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.561000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.885500</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.208500</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.623500</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.780000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               RM      target\n",
       "count  506.000000  506.000000\n",
       "mean     6.284634   22.532806\n",
       "std      0.702617    9.197104\n",
       "min      3.561000    5.000000\n",
       "25%      5.885500   17.025000\n",
       "50%      6.208500   21.200000\n",
       "75%      6.623500   25.000000\n",
       "max      8.780000   50.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (df.head(10))\n",
    "df.describe ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='RM', ylabel='target'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot.scatter('RM', 'target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Custom Linear Regression Classifier\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Load all data \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "# YOUR_CODE. select the values of feature 5 only (corresponding to 'RM') and assign to X \n",
    "# START_CODE \n",
    "#df = pd.DataFrame (X, columns= ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'])\n",
    "X= np.array(df[['RM']])\n",
    "# END_CODE \n",
    "\n",
    "X= X.reshape(-1,1) # make it 2d as for case of mutivariable\n",
    "\n",
    "# YOUR_CODE. Apply train_test_split to X and Y to get X_train, X_test, y_train, y_test\n",
    "# START_CODE \n",
    "#X = pd.DataFrame(X, columns=['RM'])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "# END_CODE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check loaded data\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape=  (379, 1)\n",
      "y_train.shape=  (379,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.605],\n",
       "       [5.927],\n",
       "       [7.267],\n",
       "       [6.471],\n",
       "       [6.782],\n",
       "       [5.983],\n",
       "       [5.875],\n",
       "       [6.824],\n",
       "       [7.249],\n",
       "       [6.086]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DON'T_CHANGE_THIS_CODE. It is used to let you check the result is correct \n",
    "print ('X_train.shape= ',X_train.shape)\n",
    "print ('y_train.shape= ',y_train.shape)\n",
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue>\n",
    "Expected output:\n",
    "</font>\n",
    "\n",
    "<br>`X_train.shape=  (379, 1)`\n",
    "<br>`y_train.shape=  (379,)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Develop expresion of h\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression_1():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "            \n",
    "    def h(self, b, w, X): \n",
    "        '''\n",
    "        :param b -  float or ndarry of shape [m,1], m - number of samples\n",
    "        :param w - ndarray of shape [1,m],  n - number of features\n",
    "        :param X - ndarray of shape [m,n], m - number of samples, n - number of features\n",
    "        '''\n",
    "        assert (X.shape[1]== w.shape[1])\n",
    "\n",
    "        # YOUR_CODE. Assign expression for h to h_res \n",
    "        # START_CODE \n",
    "        h_res= b+w*X\n",
    "        # END_CODE \n",
    "        \n",
    "        return h_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check h\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b= -0.276767596147759, \n",
      "w= [[0.581851]], \n",
      "X= \n",
      "[[ 2.14839926]\n",
      " [-1.279487  ]\n",
      " [ 0.50227689]\n",
      " [ 0.8560293 ]\n",
      " [-0.14279008]\n",
      " [ 0.11007867]\n",
      " [-0.68806479]\n",
      " [ 0.43356408]\n",
      " [ 0.510221  ]\n",
      " [-0.16513097]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.97328067],\n",
       "       [-1.02123839],\n",
       "       [ 0.01548272],\n",
       "       [ 0.22131391],\n",
       "       [-0.35985014],\n",
       "       [-0.21271821],\n",
       "       [-0.67711878],\n",
       "       [-0.0244979 ],\n",
       "       [ 0.02010501],\n",
       "       [-0.37284922]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DON'T_CHANGE_THIS_CODE. It is used to let you check the result is correct \n",
    "np.random.seed(2018)\n",
    "b_check= np.random.randn()\n",
    "w_check= np.random.randn(1,1)\n",
    "X_check= np.random.randn(10,1)\n",
    "print('b= {}, \\nw= {}, \\nX= \\n{}'.format(b_check, w_check, X_check))\n",
    "lin_reg_1 = Linear_Regression_1()\n",
    "lin_reg_1.h(b_check, w_check, X_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue>\n",
    "Expected output:\n",
    "</font>\n",
    "\n",
    "<br>`array([[ 0.97328067],\n",
    "       [-1.02123839],\n",
    "       [ 0.01548272],\n",
    "       ...\n",
    "       [-0.0244979 ],\n",
    "       [ 0.02010501],\n",
    "       [-0.37284922]])`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Develop expresion of Cost Function\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression_2():\n",
    "    '''linear regression using gradient descent\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def J (self, h, y):      \n",
    "        '''\n",
    "        :param h - ndarray of shape (m,1)\n",
    "        :param y - ndarray of shape (m,1)\n",
    "        :return expression for cost function \n",
    "        '''\n",
    "        if h.shape !=y.shape:\n",
    "            print('h.shape = {} does not match y.shape = {}.Expected {}'.format (h.shape, y.shape, (self.m,1)))\n",
    "            raise Exception('Check assertion in J')    \n",
    "   \n",
    "        # YOUR_CODE. Assign expression for J to J_res \n",
    "        # START_CODE \n",
    "        J_res = np.dot(1/(np.dot(2,m)),np.sum(np.square((h-y)))) \n",
    "        # END_CODE         \n",
    "        return J_res           \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check J\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= [[-0.21767896]\n",
      " [ 0.82145535]\n",
      " [ 1.48127781]\n",
      " [ 1.33186404]\n",
      " [-0.36186537]\n",
      " [ 0.68560883]\n",
      " [ 0.57376143]\n",
      " [ 0.28772767]\n",
      " [-0.23563426]\n",
      " [ 0.95349024]], \n",
      "h= [[-1.6896253 ]\n",
      " [-0.34494271]\n",
      " [ 0.0169049 ]\n",
      " [-0.51498352]\n",
      " [ 0.24450929]\n",
      " [-0.18931261]\n",
      " [ 2.67217242]\n",
      " [ 0.46480249]\n",
      " [ 0.84593044]\n",
      " [-0.50354158]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.897146515186598"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DON'T_CHANGE_THIS_CODE. It is used to let you check the result is correct \n",
    "np.random.seed(2019)\n",
    "m = 10 \n",
    "y_check= np.random.randn(m,1)\n",
    "h_check= np.random.randn(m,1)\n",
    "print('y= {}, \\nh= {}'.format(y_check, h_check))\n",
    "lin_reg_2 = Linear_Regression_2()\n",
    "lin_reg_2.m = m \n",
    "lin_reg_2.J(h_check, y_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue>\n",
    "Expected output:\n",
    "</font>\n",
    "\n",
    "<br>`0.897146515186598`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Develop expresion of Cost Function  derivative \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression_3():\n",
    "    def __init__(self, max_iter = 1e5, alpha = 1,eps = 1e-10, verbose= 0):\n",
    "        pass        \n",
    "\n",
    "    def h(self, b, w, X): \n",
    "        '''\n",
    "        :param b -  float or ndarry of shape [m,1], m - number of samples\n",
    "        :param w - ndarray of shape [1,m],  n - number of features\n",
    "        :param X - ndarray of shape [m,n], m - number of samples, n - number of features\n",
    "        '''\n",
    "        assert (X.shape[1]== w.shape[1])\n",
    "\n",
    "        # YOUR_CODE. Insert the expression of h developed in Linear_Regression_1\n",
    "        # START_CODE \n",
    "        h_res= b+w*X\n",
    "        # END_CODE\n",
    "\n",
    "        return h_res\n",
    "        \n",
    "    def J_derivative(self, params, X, y): \n",
    "        '''\n",
    "        :param params - tuple (b,w), where w is the 2d ndarry of shape (1,n), n- number of features \n",
    "        :param X- ndarray of shape (m, n)\n",
    "        :param y - ndarray of shape (m,1)\n",
    "        :return tuple of derivatrives of cost function by b and w\n",
    "        '''    \n",
    "        b,w = params\n",
    "        assert (w.shape == (1,self.n))                \n",
    "        h_val = self.h(b,w,X)\n",
    "        if  h_val.shape != (self.m, 1):\n",
    "            print('h.shape = {}, but expected {}'.format (h_val.shape, (self.m, 1)))\n",
    "            raise Exception('Check assertion in J_derivative')\n",
    "\n",
    "        # YOUR_CODE. Assign expressions for derivates of J by b and by w  to dJ_b and dJ_w corrrespondingly       \n",
    "        # START_CODE             \n",
    "        dJ_b= 1/m*np.sum(h_val-y) \n",
    "        dJ_w= 1/m*np.sum((h_val-y)*X) \n",
    "        # END_CODE\n",
    "        return(dJ_b, dJ_w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check cost function derivatives \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X= [[-1.76884571]\n",
      " [ 0.07555227]\n",
      " [-1.1306297 ]\n",
      " [-0.65143017]\n",
      " [-0.89311563]\n",
      " [-1.27410098]\n",
      " [-0.06115443]\n",
      " [ 0.06451384]\n",
      " [ 0.41011295]\n",
      " [-0.57288249]], \n",
      "y= [[-0.80133362]\n",
      " [ 1.31203519]\n",
      " [ 1.27469887]\n",
      " [-1.2143576 ]\n",
      " [ 0.31371941]\n",
      " [-1.44482142]\n",
      " [-0.3689613 ]\n",
      " [-0.76922658]\n",
      " [ 0.3926161 ]\n",
      " [ 0.05729383]], \n",
      "b= 2.0899788404287745 \n",
      "w= [[0.04197131]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.1904608819958713, -1.4328426209410612)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DON'T_CHANGE_THIS_CODE. It is used to let you check the result is correct \n",
    "np.random.seed(2020)\n",
    "m = 10 \n",
    "n = 1\n",
    "X_check= np.random.randn(m,n)\n",
    "y_check= np.random.randn(m,1)\n",
    "b_check= np.random.randn()\n",
    "w_check= np.random.randn(1,n)\n",
    "params = b_check,w_check \n",
    "print('X= {}, \\ny= {}, \\nb= {} \\nw= {}'.format(X_check, y_check, b_check, w_check))\n",
    "\n",
    "lin_reg_3 = Linear_Regression_3()\n",
    "lin_reg_3.m = m \n",
    "lin_reg_3.n = n \n",
    "lin_reg_3.J_derivative(params, X_check, y_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue>\n",
    "Expected output:\n",
    "</font>\n",
    "\n",
    "`(2.1904608819958713, -1.4328426209410612)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Develop gradient descent \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression_4():\n",
    "    '''\n",
    "    linear regression using gradient descent\n",
    "    '''\n",
    "    def __init__(self, max_iter = 1e5, alpha = 0.01,eps = 1e-10, verbose= 0):\n",
    "        '''\n",
    "        :param verbose: set 1 to display more details of J val changes\n",
    "        '''\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        self.verbose = verbose       \n",
    "        \n",
    "    def h(self, b, w, X): \n",
    "        '''\n",
    "        :param b -  float or ndarry of shape [m,1], m - number of samples\n",
    "        :param w - ndarray of shape [1,m],  n - number of features\n",
    "        :param X - ndarray of shape [m,n], m - number of samples, n - number of features\n",
    "        '''\n",
    "        assert (X.shape[1]== w.shape[1])\n",
    "\n",
    "        # YOUR_CODE. Insert the expression of h developed in Linear_Regression_1\n",
    "        # START_CODE \n",
    "        h_res= b+w*X\n",
    "        # END_CODE \n",
    "        \n",
    "        if h_res.shape != (X.shape[0],1):\n",
    "            print('h.shape = {} but expected {}'.format (h_res.shape,  (self.m,1)))\n",
    "            raise Exception('Check assertion in h')    \n",
    "        return h_res\n",
    "\n",
    "    def J (self, h, y):      \n",
    "        '''\n",
    "        :param h - ndarray of shape (m,1)\n",
    "        :param y - ndarray of shape (m,1)\n",
    "        :return expression for cost function \n",
    "        '''\n",
    "        if h.shape !=y.shape:\n",
    "            print('h.shape = {} does not match y.shape = {}.Expected {}'.format (h.shape, y.shape, (self.m,1)))\n",
    "            raise Exception('Check assertion in J')   \n",
    "        # YOUR_CODE. Insert the expression of J developed in Linear_Regression_2\n",
    "        # START_CODE \n",
    "        J_res= 1/(2*m)*np.sum((h-y)**2) \n",
    "        # END_CODE \n",
    "\n",
    "        return J_res\n",
    "        \n",
    "    def J_derivative(self, params, X, y): \n",
    "        '''\n",
    "        :param params - tuple (b,w), where w is the 2d ndarry of shape (1,n), n- number of features \n",
    "        :param X- ndarray of shape (m, n)\n",
    "        :param y - ndarray of shape (m,1)\n",
    "        :return tuple of derivatrives of cost function by b and w\n",
    "        '''\n",
    "      \n",
    "        b,w = params\n",
    "        assert (w.shape == (1,self.n))                \n",
    "        h_val = self.h(b,w,X)\n",
    "        if  h_val.shape != (self.m, 1):\n",
    "            print('h.shape = {}, but expected {}'.format (h_val.shape, (self.m, 1)))\n",
    "            raise Exception('Check assertion in J_derivative')\n",
    "        \n",
    "        # YOUR_CODE. Insert the expressions for derivates of J by b and by w to dJ_b and dJ_w developed in Linear_Regression_3\n",
    "        # START_CODE             \n",
    "        dJ_b= 1/m*np.sum(h_val-y)\n",
    "        dJ_w= 1/m*np.sum((h_val-y)*X)\n",
    "        # END_CODE\n",
    "        \n",
    "        return (dJ_b, dJ_w)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        :param X - ndarray training set of shape [m,n], m - number of samples, n - number of features\n",
    "        :param y - ndarray - 1d array \n",
    "        :return: True in case of successful fit \n",
    "        '''      \n",
    "        if self.verbose: \n",
    "            print ('Running gradient descent with alpha = {}, eps= {}, max_iter= {}'.format(\n",
    "                self.alpha, self.eps, self.max_iter))\n",
    "        self.m,self.n= X.shape # number of samples, number of features  \n",
    "        y = y.reshape(self.m,1) # make it 2 d to make sure it corresponds to h_val\n",
    "        b = 0 # init intercept with 0\n",
    "        w= np.zeros(self.n).reshape(1,-1) # make sure it's shape is [1,n]\n",
    "        params = (b,w)\n",
    "        \n",
    "        self.J_hist=[-1] # used for keeping J values. Init with -1 to avoid 0 at first iter\n",
    "        continue_iter = True # flag to continue next iter (grad desc step)\n",
    "        iter_number =0 # used for limit by max_iter\n",
    "\n",
    "        while continue_iter:            \n",
    "            # Do step of gradient descent    \n",
    "            # YOUR_CODE. Develop one step of gradien descent \n",
    "            # START_CODE \n",
    "            dJ_b, dJ_w = self.J_derivative(params, X, y)\n",
    "            b= b - self.alpha* dJ_b\n",
    "            w= w - self.alpha*dJ_w\n",
    "            params= b,w\n",
    "            # END_CODE \n",
    "            \n",
    "            # keep history of J values\n",
    "            self.J_hist.append(self.J(self.h(b, w, X), y))\n",
    "            if self.verbose:\n",
    "                print ('b = {}, w= {}, J= {}'.format(b,w,self.J_hist[-1]))\n",
    "            # check criteria of exit the loop (finish grad desc)\n",
    "            if self.max_iter and iter_number> self.max_iter: # if max_iter is provided and limit succeeded\n",
    "                continue_iter = False\n",
    "            elif np.abs(self.J_hist[iter_number-1] - self.J_hist[iter_number])< self.eps: # if accuracy is succeeded\n",
    "                continue_iter = False\n",
    "            iter_number += 1\n",
    "            \n",
    "        # store the final params to further using \n",
    "        self.intercept_, self.coef_= params        \n",
    "        return True        \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check gradient descent\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X= [[ 1.48860905]\n",
      " [ 0.67601087]\n",
      " [-0.41845137]\n",
      " [-0.80652081]\n",
      " [ 0.55587583]\n",
      " [-0.70550429]\n",
      " [ 1.13085826]\n",
      " [ 0.64500184]\n",
      " [ 0.10641374]\n",
      " [ 0.42215483]], \n",
      "y= [[ 0.12420684]\n",
      " [-0.83795346]\n",
      " [ 0.4090157 ]\n",
      " [ 0.10275122]\n",
      " [-1.90772239]\n",
      " [ 1.1002243 ]\n",
      " [-1.40232506]\n",
      " [-0.22508127]\n",
      " [-1.33620597]\n",
      " [ 0.30372151]]\n",
      "Running gradient descent with alpha = 1, eps= 1e-10, max_iter= 5\n",
      "b = -0.36693685587288444, w= [[-0.4217246]], J= 0.33976525493056825\n",
      "b = -0.23643637277401236, w= [[-0.46886908]], J= 0.3278115023016167\n",
      "b = -0.22184776004990137, w= [[-0.52721539]], J= 0.3250909705515032\n",
      "b = -0.20379279582278398, w= [[-0.55396166]], J= 0.32428457786538833\n",
      "b = -0.19551630227029396, w= [[-0.5697399]], J= 0.32403801171263197\n",
      "b = -0.19063380881762437, w= [[-0.57831305]], J= 0.3239623872203208\n",
      "b = -0.18798089094052142, w= [[-0.58309057]], J= 0.3239391853771439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DON'T_CHANGE_THIS_CODE. It is used to let you check the result is correct \n",
    "np.random.seed(2021)\n",
    "m = 10 \n",
    "n = 1\n",
    "X_check= np.random.randn(m,n)\n",
    "y_check= np.random.randn(m,1)\n",
    "print('X= {}, \\ny= {}'.format(X_check, y_check))\n",
    "lin_reg_4 = Linear_Regression_4(alpha = 1, max_iter = 5, verbose=1)\n",
    "lin_reg_4.fit(X_check, y_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue>\n",
    "Expected output:\n",
    "</font>\n",
    "\n",
    "`Running gradient descent with alpha = 1, eps= 1e-10, max_iter= 5\n",
    "b = -0.36693685587288444, w= [[-0.4217246]], J= 0.33976525493056825\n",
    "b = -0.23643637277401236, w= [[-0.46886908]], J= 0.3278115023016167\n",
    "b = -0.22184776004990137, w= [[-0.52721539]], J= 0.3250909705515032\n",
    "b = -0.20379279582278398, w= [[-0.55396166]], J= 0.32428457786538833\n",
    "b = -0.19551630227029396, w= [[-0.5697399]], J= 0.32403801171263197\n",
    "b = -0.19063380881762437, w= [[-0.57831305]], J= 0.3239623872203208\n",
    "b = -0.18798089094052142, w= [[-0.58309057]], J= 0.3239391853771439`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Launch liner regression learning on real values. \n",
    "Please review addtional already implemented functions: draw_cost_changes(), predict() and score()\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression():\n",
    "    '''\n",
    "    linear regression using gradient descent\n",
    "    '''\n",
    "    def __init__(self, max_iter = 1e5, alpha = 0.01, eps = 1e-10, verbose= 0):\n",
    "        '''\n",
    "        :param verbose: set 1 to display more details of J val changes\n",
    "        '''\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        self.verbose = verbose       \n",
    "        \n",
    "    def h(self, b, w, X): \n",
    "        '''\n",
    "        :param b -  float or ndarry of shape [m,1], m - number of samples\n",
    "        :param w - ndarray of shape [1,m],  n - number of features\n",
    "        :param X - ndarray of shape [m,n], m - number of samples, n - number of features\n",
    "        '''\n",
    "        assert (X.shape[1]== w.shape[1])\n",
    "\n",
    "        # YOUR_CODE. Insert the expression of h developed in Linear_Regression_1\n",
    "        # START_CODE \n",
    "        h_res= b+w*X\n",
    "        # END_CODE \n",
    "        \n",
    "        if h_res.shape != (X.shape[0],1):\n",
    "            print('h.shape = {} but expected {}'.format (h_res.shape,  (self.m,1)))\n",
    "            raise Exception('Check assertion in h')    \n",
    "        return h_res\n",
    "\n",
    "    def J (self, h, y):      \n",
    "        '''\n",
    "        :param h - ndarray of shape (m,1)\n",
    "        :param y - ndarray of shape (m,1)\n",
    "        :return expression for cost function \n",
    "        '''\n",
    "        if h.shape !=y.shape:\n",
    "            print('h.shape = {} does not match y.shape = {}.Expected {}'.format (h.shape, y.shape, (self.m,1)))\n",
    "            raise Exception('Check assertion in J')   \n",
    "        # YOUR_CODE. Insert the expression of J developed in Linear_Regression_2\n",
    "        # START_CODE \n",
    "        J_res= 1/(2*m)*np.sum((h-y)**2)\n",
    "        # END_CODE \n",
    "\n",
    "        return J_res\n",
    "        \n",
    "    def J_derivative(self, params, X, y): \n",
    "        '''\n",
    "        :param params - tuple (b,w), where w is the 2d ndarry of shape (1,n), n- number of features \n",
    "        :param X- ndarray of shape (m, n)\n",
    "        :param y - ndarray of shape (m,1)\n",
    "        :return tuple of derivatrives of cost function by b and w\n",
    "        '''\n",
    "      \n",
    "        b,w = params\n",
    "        assert (w.shape == (1,self.n))                \n",
    "        h_val = self.h(b,w,X)\n",
    "        if  h_val.shape != (self.m, 1):\n",
    "            print('h.shape = {}, but expected {}'.format (h_val.shape, (self.m, 1)))\n",
    "            raise Exception('Check assertion in J_derivative')\n",
    "        \n",
    "        # YOUR_CODE. Insert the expressions for derivates of J by b and by w to dJ_b and dJ_w developed in Linear_Regression_3\n",
    "        # START_CODE             \n",
    "        dJ_b= 1/m*np.sum(h_val-y)\n",
    "        dJ_w= 1/m*np.sum((h_val-y)*X)\n",
    "        # END_CODE\n",
    "        \n",
    "        return (dJ_b, dJ_w)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        :param X - ndarray training set of shape [m,n], m - number of samples, n - number of features\n",
    "        :param y - ndarray - 1d array \n",
    "        :return: True in case of successful fit \n",
    "        '''      \n",
    "        if self.verbose: \n",
    "            print ('Running gradient descent with alpha = {}, eps= {}, max_iter= {}'.format(\n",
    "                self.alpha, self.eps, self.max_iter))\n",
    "        self.m,self.n= X.shape # number of samples, number of features  \n",
    "        y = y.reshape(self.m,1) # make it 2 d to make sure it corresponds to h_val\n",
    "        b = 0 # init intercept with 0\n",
    "        w= np.zeros(self.n).reshape(1,-1) # make sure it's shape is [1,n]\n",
    "        params = (b,w)\n",
    "        \n",
    "        self.J_hist=[-1] # used for keeping J values. Init with -1 to avoid 0 at first iter\n",
    "        continue_iter = True # flag to continue next iter (grad desc step)\n",
    "        iter_number =0 # used for limit by max_iter\n",
    "\n",
    "        while continue_iter:            \n",
    "            # Do step of gradient descent    \n",
    "            # YOUR_CODE. Insert one step of gradien descent developed in Linear_Regression_4 \n",
    "            # START_CODE \n",
    "            dJ_b, dJ_w = self.J_derivative(params, X, y)\n",
    "            b= b- self.alpha*dJ_b \n",
    "            w= w- self.alpha*dJ_w \n",
    "            params= (b,w)\n",
    "            # END_CODE \n",
    "            \n",
    "            # keep history of J values\n",
    "            self.J_hist.append(self.J(self.h(b, w, X), y))\n",
    "            if self.verbose:\n",
    "                print ('b = {}, w= {}, J= {}'.format(b,w,self.J_hist[-1]))\n",
    "            # check criteria of exit the loop (finish grad desc)\n",
    "            if self.max_iter and iter_number> self.max_iter: # if max_iter is provided and limit succeeded\n",
    "                continue_iter = False\n",
    "            elif np.abs(self.J_hist[iter_number-1] - self.J_hist[iter_number])< self.eps: # if accuracy is succeeded\n",
    "                continue_iter = False\n",
    "            iter_number += 1\n",
    "            \n",
    "        # store the final params to further using \n",
    "        self.intercept_, self.coef_= params        \n",
    "        return True        \n",
    "        \n",
    "    def draw_cost_changes(self):        \n",
    "        J_hist= self.J_hist[1:]\n",
    "        plt.figure()\n",
    "        plt.scatter(np.arange(0,len(J_hist)),J_hist,s=20,marker='.',c='b')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Cost function J value')\n",
    "        title_str = 'Complited: {}, alpha ={}, max_iter={}, eps={}'.format( len(self.J_hist)-2, self.alpha, self.max_iter,self.eps)\n",
    "        # Note: len(J_hist)-2) due to first one is -1 (was not iteration), iter + 1  at the end  of the gradient loop\n",
    "        plt.title(title_str)\n",
    " \n",
    "\n",
    "    def predict(self, X): \n",
    "        '''\n",
    "        :param X - ndarray of shape (?,n)\n",
    "        :return \n",
    "        '''\n",
    "        return self.h(self.intercept_, self.coef_, X)\n",
    "        \n",
    "   \n",
    "    def score(self, X_test, y_test):\n",
    "        '''\n",
    "        :param X_test - ndarray testing set or any for prediction of shape [?,n], ? - number of samples, n - number of features\n",
    "        :param y_test - ndarray - 1d array \n",
    "        :return R2 score of y_test and prediction for X_test\n",
    "        '''\n",
    "        z= self.predict(X_test)\n",
    "        from sklearn.metrics.scorer import r2_score\n",
    "        return (r2_score(y_test, z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check results\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape=  (379, 1)\n",
      "y_train.shape=  (379,)\n",
      "X_train= \n",
      "[[5.605]\n",
      " [5.927]\n",
      " [7.267]\n",
      " [6.471]\n",
      " [6.782]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leleka\\.conda\\envs\\Test1\\lib\\site-packages\\numpy\\core\\fromnumeric.py:87: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "<ipython-input-17-9f7db9ee8880>:43: RuntimeWarning: overflow encountered in square\n",
      "  J_res= 1/(2*m)*np.sum((h-y)**2)\n",
      "<ipython-input-17-9f7db9ee8880>:107: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  elif np.abs(self.J_hist[iter_number-1] - self.J_hist[iter_number])< self.eps: # if accuracy is succeeded\n",
      "<ipython-input-17-9f7db9ee8880>:95: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  b= b- self.alpha*dJ_b\n",
      "<ipython-input-17-9f7db9ee8880>:96: RuntimeWarning: invalid value encountered in subtract\n",
      "  w= w- self.alpha*dJ_w\n",
      "C:\\Users\\Leleka\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-668feffd44b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mlin_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mlin_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_cost_changes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'R2 Score ='\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlin_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                        \u001b[1;31m#??????\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'b: {}, w= {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlin_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlin_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m           \u001b[1;31m#??????\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-9f7db9ee8880>\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X_test, y_test)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0mz\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscorer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36mr2_score\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;33m-\u001b[0m\u001b[1;36m3.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m     \"\"\"\n\u001b[1;32m--> 576\u001b[1;33m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[0;32m    577\u001b[0m         y_true, y_pred, multioutput)\n\u001b[0;32m    578\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m             _assert_all_finite(array,\n\u001b[0m\u001b[0;32m    578\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m     55\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     56\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                     (type_err,\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DON'T_CHANGE_THIS_CODE. It is used to let you check the result is correct \n",
    "\n",
    "print ('X_train.shape= ',X_train.shape)\n",
    "print ('y_train.shape= ',y_train.shape)\n",
    "print ('X_train= \\n{}'.format (X_train[:5,:]))\n",
    "lin_reg = Linear_Regression(alpha= 0.01, verbose=0, eps=1e-8)\n",
    "lin_reg.fit (X_train, y_train)\n",
    "lin_reg.draw_cost_changes()\n",
    "print ('R2 Score =', lin_reg.score(X_test, y_test))                        #??????\n",
    "print ('b: {}, w= {}'.format(lin_reg.intercept_, lin_reg.coef_))           #?????? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue>\n",
    "Expected output:\n",
    "</font>\n",
    "\n",
    "`\n",
    "R2 Score = 0.5651980288912308\n",
    "b: -32.8552746555104, w= [[8.7874915]]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Draw scatter and prediction for one feature\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train.shape[1]>1:\n",
    "    raise Exception ('Select single feature to plot')\n",
    "plt.figure()\n",
    "plt.scatter(X_train, y_train)\n",
    "x_line= np.array([np.min(X_train), np.max(X_train)])\n",
    "z_line = lin_reg.predict(x_line.reshape(-1,1))\n",
    "plt.plot(x_line, z_line, '-', c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Using normalization\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled= scaler.fit_transform(X_train)\n",
    "X_test_scaled= scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = Linear_Regression(alpha= 0.01, verbose=0, eps=1e-8)\n",
    "lin_reg.fit (X_train_scaled, y_train)\n",
    "print ('R2 Score =',lin_reg.score(X_test_scaled, y_test))\n",
    "lin_reg.draw_cost_changes()\n",
    "print ('b: {}, w= {}'.format(lin_reg.intercept_, lin_reg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue>\n",
    "Expected output:\n",
    "</font>\n",
    "\n",
    "`\n",
    "b: 22.28982200153653, w= [[6.23074394]]\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: How faster it converges "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Compare with sklearn\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg_sklearn = LinearRegression().fit(X_train_scaled, y_train)\n",
    "lin_reg_sklearn.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue>\n",
    "Expected output:\n",
    "</font>\n",
    "\n",
    "`\n",
    "0.5692801665656615\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Run on real data\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed= 2021\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test=  train_test_split(X, y, random_state=2018)\n",
    "print ('X_train.shape= ',X_train.shape)\n",
    "print ('y_train.shape= ',y_train.shape)\n",
    "X_train[:5]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled= scaler.fit_transform(X_train)\n",
    "X_test_scaled= scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lin_reg = Linear_Regression(alpha= 0.1, verbose=0, eps=1e-5, max_iter=100000)\n",
    "lin_reg.fit (X_train_scaled, y_train)\n",
    "lin_reg.draw_cost_changes()\n",
    "print ('R2 training Score =', lin_reg.score(X_train_scaled, y_train))\n",
    "print ('R2 Score =', lin_reg.score(X_test_scaled, y_test))\n",
    "print ('b: {}, w= {}'.format(lin_reg.intercept_, lin_reg.coef_)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue>\n",
    "Expected output:\n",
    "</font>\n",
    "\n",
    "`\n",
    "R2 training Score = 0.7283111795119549\n",
    "R2 Score = 0.7714399743645595\n",
    "b: 22.199472295514532, w= [[-6.71888107e-01  1.10023856e+00  4.11947599e-03  8.26282274e-01\n",
    "  -2.22625058e+00  2.43471682e+00  2.54149326e-01 -3.29472715e+00\n",
    "   2.45132782e+00 -1.99309805e+00 -1.95019870e+00  7.67364288e-01\n",
    "  -4.20581658e+00]]\n",
    "`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
