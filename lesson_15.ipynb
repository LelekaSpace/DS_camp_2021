{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "## Home Task \n",
    "\n",
    "</font>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "### Load data \n",
    "\n",
    "</font>\n",
    "\n",
    "[Sentiment Analysis Dataset](https://www.kaggle.com/sonaam1234/sentimentdata)\n",
    "\n",
    "alternative source: \n",
    "<br>\n",
    "[rt-polaritydata](https://github.com/dennybritz/cnn-text-classification-tf/tree/master/data/rt-polaritydata)\n",
    "\n",
    "alternative source: \n",
    "<br>\n",
    "[Movie Review Data](http://www.cs.cornell.edu/people/pabo/movie-review-data)\n",
    "\n",
    "Each line in these two files corresponds to a single snippet (usually containing roughly one single sentence); all snippets are down-cased.  \n",
    "[More info about dataset](https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.README.1.0.txt)\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "fn='rt-polarity.neg'\r\n",
    "with open(fn, \"r\",encoding='utf-8', errors='ignore') as f: # some invalid symbols encountered \r\n",
    "    content = f.read()  \r\n",
    "texts_neg=  content.splitlines()\r\n",
    "print ('len of texts_neg = {:,}'.format (len(texts_neg)))\r\n",
    "for review in texts_neg[:5]:\r\n",
    "    print ( '\\n', review)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "len of texts_neg = 5,331\n",
      "\n",
      " simplistic , silly and tedious . \n",
      "\n",
      " it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
      "\n",
      " exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n",
      "\n",
      " [garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n",
      "\n",
      " a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "fn='rt-polarity.pos'\r\n",
    "\r\n",
    "with open(fn, \"r\",encoding='utf-8', errors='ignore') as f:\r\n",
    "    content = f.read()\r\n",
    "texts_pos=  content.splitlines()\r\n",
    "print ('len of texts_pos = {:,}'.format (len(texts_pos)))\r\n",
    "for review in texts_pos[:5]:\r\n",
    "    print ('\\n', review)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "len of texts_pos = 5,331\n",
      "\n",
      " the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      " the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
      "\n",
      " effective but too-tepid biopic\n",
      "\n",
      " if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n",
      "\n",
      " emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import nltk\r\n",
    "from nltk.corpus import movie_reviews \r\n",
    "from nltk.corpus import stopwords\r\n",
    "import nltk\r\n",
    "from nltk.tokenize import RegexpTokenizer\r\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "t = texts_pos+texts_neg\r\n",
    "t[0]\r\n",
    "te = ''.join(t)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def preprocess(text): \r\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') \r\n",
    "    return tokenizer.tokenize(text.lower())\r\n",
    "\r\n",
    "all_words = preprocess(te)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "all_words=nltk.FreqDist(all_words)\r\n",
    "most_common_words = list(zip(*all_words.most_common()))[0]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def remove_stop_words(words):\r\n",
    "    stop_words = set(stopwords.words('english'))  \r\n",
    "    return [w for w in words if w not in stop_words]\r\n",
    "most_common_words_filtered = remove_stop_words(most_common_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "word_features = most_common_words_filtered [:3000]\r\n",
    "documents = [(list(movie_reviews.words(file_id)), category) \r\n",
    "             for category in movie_reviews.categories() \r\n",
    "             for file_id in movie_reviews.fileids(category)]\r\n",
    "\r\n",
    "random.shuffle(documents) \r\n",
    "documents= documents[:500]\r\n",
    "\r\n",
    "def find_features(review_tokens):\r\n",
    "    return {w: w in set(review_tokens) for w in word_features} \r\n",
    "\r\n",
    "data_set= [(find_features(review_tokens), category) for (review_tokens, category) in documents]\r\n",
    "split_on = int(len(data_set)*.8)\r\n",
    "X_y_train= data_set[:split_on]\r\n",
    "X_y_test = data_set[split_on:]\r\n",
    "\r\n",
    "clf= nltk.NaiveBayesClassifier.train(X_y_train) \r\n",
    "\r\n",
    "\r\n",
    "print(nltk.classify.accuracy(clf, X_y_test)*100)\r\n",
    "\r\n",
    "clf.show_most_informative_features(15)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "85.0\n",
      "Most Informative Features\n",
      "             masterpiece = True              pos : neg    =      9.0 : 1.0\n",
      "                 unfunny = True              neg : pos    =      8.9 : 1.0\n",
      "                   blade = True              pos : neg    =      8.3 : 1.0\n",
      "              remembered = True              pos : neg    =      7.6 : 1.0\n",
      "               laughable = True              neg : pos    =      7.1 : 1.0\n",
      "            conventional = True              pos : neg    =      6.9 : 1.0\n",
      "                 decades = True              pos : neg    =      6.9 : 1.0\n",
      "                    dull = True              neg : pos    =      6.8 : 1.0\n",
      "                    rare = True              pos : neg    =      6.7 : 1.0\n",
      "                    lame = True              neg : pos    =      6.5 : 1.0\n",
      "                    adam = True              neg : pos    =      6.5 : 1.0\n",
      "                   grade = True              neg : pos    =      6.5 : 1.0\n",
      "             outstanding = True              pos : neg    =      6.3 : 1.0\n",
      "             magnificent = True              pos : neg    =      6.1 : 1.0\n",
      "                  runner = True              pos : neg    =      6.1 : 1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "col = ['Reviews', 'Rating_binary']\r\n",
    "df = pd.DataFrame(columns=col)\r\n",
    "\r\n",
    "def rew(rew,rat):\r\n",
    "\r\n",
    "    global col\r\n",
    "    global df\r\n",
    "\r\n",
    "    list_col = [rew, rat]\r\n",
    "\r\n",
    "    F = pd.DataFrame([list_col], columns=col)\r\n",
    "    df = df.append(F)\r\n",
    "\r\n",
    "for r in texts_pos:\r\n",
    "    rew(r, 1)\r\n",
    "\r\n",
    "for r in texts_neg:\r\n",
    "    rew(r, 0)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Reviews'],df['Rating_binary'],random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \r\n",
    "vect = CountVectorizer().fit(X_train) \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "X_train_vectorized = vect.transform(X_train) \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "df = pd.DataFrame(X_train_vectorized[0].toarray(), index= ['value']).T\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "[vect.get_feature_names()[index] for index in df[df['value']>0].index.values]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['about',\n",
       " 'as',\n",
       " 'been',\n",
       " 'cutting',\n",
       " 'fresh',\n",
       " 'have',\n",
       " 'hollywood',\n",
       " 'instead',\n",
       " 'is',\n",
       " 'issue',\n",
       " 'last',\n",
       " 'of',\n",
       " 'satire',\n",
       " 'should',\n",
       " 'variety',\n",
       " 'week',\n",
       " 'what']"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "from sklearn.metrics import f1_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "y_train1=y_train.astype('int')\r\n",
    "y_test1=y_test.astype('int')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "clf = LogisticRegression(max_iter=2000).fit(X_train_vectorized, y_train1) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "predictions = clf.predict(vect.transform(X_test)) \r\n",
    "print('f1: ', f1_score(y_test1, predictions)) \r\n",
    "scores = clf.decision_function(vect.transform(X_test)) \r\n",
    "print('AUC: ', roc_auc_score(y_test1, scores)) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "f1:  0.7782805429864252\n",
      "AUC:  0.8472062608495887\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "feature_names = np.array(vect.get_feature_names())\r\n",
    "sorted_coef_index = clf.coef_[0].argsort()\r\n",
    "clf.coef_.shape, clf.coef_[0].shape, sorted(clf.coef_[0])[:10], sorted(clf.coef_[0])[-11:-1], "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1, 16021),\n",
       " (16021,),\n",
       " [-2.287303923226548,\n",
       "  -1.834743207989059,\n",
       "  -1.8051071449693172,\n",
       "  -1.660050440471349,\n",
       "  -1.6233773186487392,\n",
       "  -1.619425827528814,\n",
       "  -1.5768822688401176,\n",
       "  -1.5561489186290967,\n",
       "  -1.5224796011119384,\n",
       "  -1.5174272775761568],\n",
       " [1.3894967026182408,\n",
       "  1.4084225640363601,\n",
       "  1.4219326279639313,\n",
       "  1.4612554578947061,\n",
       "  1.5041495932498896,\n",
       "  1.5069489085409427,\n",
       "  1.532535263757047,\n",
       "  1.6376105361852695,\n",
       "  1.6412227149458103,\n",
       "  1.6604110815247335])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "print('Smallest coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\r\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Smallest coefs:\n",
      "['dull' 'boring' 'waste' 'problem' 'worst' 'neither' 'bore' 'tv' 'suffers'\n",
      " 'supposed']\n",
      "\n",
      "Largest Coefs: \n",
      "['masterpiece' 'thanks' 'enjoyable' 'entertaining' 'powerful' 'remarkable'\n",
      " 'solid' 'engrossing' 'provides' 'skin']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color = green >\n",
    "\n",
    "## Learn more\n",
    "</font>\n",
    "\n",
    "sklearn.feature_extraction.text.CountVectorizer\n",
    "<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "Bag-of-words model\n",
    "<br>\n",
    "https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "\n",
    "tfâ€“idf\n",
    "<br>\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "\n",
    "sklearn.feature_extraction.text.TfidfVectorizer\n",
    "<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "Applied Text Mining in Python\n",
    "<br>\n",
    "https://www.coursera.org/learn/python-text-mining/home/welcome\n",
    "\n",
    "Natural Language Processing tutorial\n",
    "<br>\n",
    "https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('Test1': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "ae22ff7b4990f16f2107899c7d411110cc54095c4c6bd2738b6d044e1b22e01d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}